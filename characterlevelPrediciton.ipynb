{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgsDtqiHnbAW8GI7Y8Yoec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishektripathi66/RecomendationSystem/blob/main/characterlevelPrediciton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uVtiCXveBP8",
        "outputId": "c702a5d5-67a5-4370-c834-92f703b1b3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([   81,  2200,  3697,  8808, 50257, 50257, 50257]), 'attention_mask': tensor([1, 1, 1, 1, 0, 0, 0]), 'labels': tensor([   81,  2200,  3697,  8808, 50257, 50257, 50257])}\n",
            "Epoch 1, Loss: 37.49110164642334\n",
            "Epoch 2, Loss: 5.981650543212891\n",
            "Epoch 3, Loss: 3.753898525238037\n",
            "Epoch 4, Loss: 2.8920857906341553\n",
            "Epoch 5, Loss: 2.4336049556732178\n",
            "Epoch 6, Loss: 2.1835779428482054\n",
            "Epoch 7, Loss: 1.9286766290664672\n",
            "Epoch 8, Loss: 1.6818181991577148\n",
            "Epoch 9, Loss: 1.5303081750869751\n",
            "Epoch 10, Loss: 1.2995360136032104\n",
            "Epoch 11, Loss: 1.1748485565185547\n",
            "Epoch 12, Loss: 1.0438292145729064\n",
            "Epoch 13, Loss: 0.9968490958213806\n",
            "Epoch 14, Loss: 0.855038583278656\n",
            "Epoch 15, Loss: 0.7869224429130555\n",
            "Epoch 16, Loss: 0.6829974412918091\n",
            "Epoch 17, Loss: 0.7049263000488282\n",
            "Epoch 18, Loss: 0.6196663618087769\n",
            "Epoch 19, Loss: 0.6181263446807861\n",
            "Epoch 20, Loss: 0.6203503727912902\n",
            "Epoch 21, Loss: 0.56784508228302\n",
            "Epoch 22, Loss: 0.6478658676147461\n",
            "Epoch 23, Loss: 0.5848889231681824\n",
            "Epoch 24, Loss: 0.5414796948432923\n",
            "Epoch 25, Loss: 0.5889170587062835\n",
            "Epoch 26, Loss: 0.5401528298854827\n",
            "Epoch 27, Loss: 0.530787217617035\n",
            "Epoch 28, Loss: 0.5539760053157806\n",
            "Epoch 29, Loss: 0.542850649356842\n",
            "Epoch 30, Loss: 0.5871961712837219\n",
            "Epoch 31, Loss: 0.5853410243988038\n",
            "Epoch 32, Loss: 0.5088658511638642\n",
            "Epoch 33, Loss: 0.5563858091831207\n",
            "Epoch 34, Loss: 0.5156743407249451\n",
            "Epoch 35, Loss: 0.5685816526412963\n",
            "Epoch 36, Loss: 0.5829698383808136\n",
            "Epoch 37, Loss: 0.5485824942588806\n",
            "Epoch 38, Loss: 0.5251932621002198\n",
            "Epoch 39, Loss: 0.5649462401866913\n",
            "Epoch 40, Loss: 0.5133714854717255\n",
            "Epoch 41, Loss: 0.5308857321739197\n",
            "Epoch 42, Loss: 0.5165647983551025\n",
            "Epoch 43, Loss: 0.5218354642391205\n",
            "Epoch 44, Loss: 0.5160148024559021\n",
            "Epoch 45, Loss: 0.536751788854599\n",
            "Epoch 46, Loss: 0.5445212066173554\n",
            "Epoch 47, Loss: 0.5981541693210601\n",
            "Epoch 48, Loss: 0.5286739528179168\n",
            "Epoch 49, Loss: 0.5252306580543518\n",
            "Epoch 50, Loss: 0.5760423898696899\n",
            "Epoch 51, Loss: 0.5191849350929261\n",
            "Epoch 52, Loss: 0.5190306425094604\n",
            "Epoch 53, Loss: 0.532953405380249\n",
            "Epoch 54, Loss: 0.5408011615276337\n",
            "Epoch 55, Loss: 0.5094798684120179\n",
            "Epoch 56, Loss: 0.5239296793937683\n",
            "Epoch 57, Loss: 0.5416362404823303\n",
            "Epoch 58, Loss: 0.5178665041923523\n",
            "Epoch 59, Loss: 0.5217930793762207\n",
            "Epoch 60, Loss: 0.5153458178043365\n",
            "Epoch 61, Loss: 0.5189782023429871\n",
            "Epoch 62, Loss: 0.5447096943855285\n",
            "Epoch 63, Loss: 0.5476889491081238\n",
            "Epoch 64, Loss: 0.49609134197235105\n",
            "Epoch 65, Loss: 0.5336293160915375\n",
            "Epoch 66, Loss: 0.49522196054458617\n",
            "Epoch 67, Loss: 0.5056571304798126\n",
            "Epoch 68, Loss: 0.507507598400116\n",
            "Epoch 69, Loss: 0.5169176638126374\n",
            "Epoch 70, Loss: 0.46519707441329955\n",
            "Epoch 71, Loss: 0.5472024738788605\n",
            "Epoch 72, Loss: 0.5207063615322113\n",
            "Epoch 73, Loss: 0.5299324572086335\n",
            "Epoch 74, Loss: 0.4671985864639282\n",
            "Epoch 75, Loss: 0.529452633857727\n",
            "Epoch 76, Loss: 0.5201285123825073\n",
            "Epoch 77, Loss: 0.49993216395378115\n",
            "Epoch 78, Loss: 0.529231709241867\n",
            "Epoch 79, Loss: 0.5073998808860779\n",
            "Epoch 80, Loss: 0.5362890303134918\n",
            "Epoch 81, Loss: 0.5319770574569702\n",
            "Epoch 82, Loss: 0.5770164549350738\n",
            "Epoch 83, Loss: 0.5447710752487183\n",
            "Epoch 84, Loss: 0.5507980823516846\n",
            "Epoch 85, Loss: 0.5170320034027099\n",
            "Epoch 86, Loss: 0.4858682334423065\n",
            "Epoch 87, Loss: 0.5104611635208129\n",
            "Epoch 88, Loss: 0.48443024754524233\n",
            "Epoch 89, Loss: 0.4938318967819214\n",
            "Epoch 90, Loss: 0.5040762603282929\n",
            "Epoch 91, Loss: 0.4967217922210693\n",
            "Epoch 92, Loss: 0.4932953119277954\n",
            "Epoch 93, Loss: 0.45831478834152223\n",
            "Epoch 94, Loss: 0.48049787878990174\n",
            "Epoch 95, Loss: 0.48408976197242737\n",
            "Epoch 96, Loss: 0.496482914686203\n",
            "Epoch 97, Loss: 0.4752365529537201\n",
            "Epoch 98, Loss: 0.49524328112602234\n",
            "Epoch 99, Loss: 0.4796127796173096\n",
            "Epoch 100, Loss: 0.48954278230667114\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('word_prediction_model/tokenizer_config.json',\n",
              " 'word_prediction_model/special_tokens_map.json',\n",
              " 'word_prediction_model/vocab.json',\n",
              " 'word_prediction_model/merges.txt',\n",
              " 'word_prediction_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = \"testdata.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "# Preprocess the data\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'eos_token': '[PAD]'})  # Add padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
        "\n",
        "# Tokenize the input and next words\n",
        "input_texts = df[\"prefix\"].tolist()\n",
        "next_words = df[\"next_word\"].tolist()\n",
        "encoded_data = tokenizer(input_texts, text_pair=next_words, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encoded_data):\n",
        "        self.input_ids = encoded_data[\"input_ids\"]\n",
        "        self.attention_mask = encoded_data[\"attention_mask\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.input_ids[idx]  # Use input_ids as labels\n",
        "        }\n",
        "\n",
        "train_dataset = CustomDataset(encoded_data)\n",
        "\n",
        "# Define the model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Fine-tune the model with additional adjustments\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)  # Adjusted learning rate\n",
        "\n",
        "model.train()\n",
        "for epoch in range(100):  # Train for 5 epochs\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"word_prediction_model\")\n",
        "tokenizer.save_pretrained(\"word_prediction_model\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"word_prediction_model\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"word_prediction_model\")\n",
        "\n",
        "# Function to generate predictions for custom input prefixes\n",
        "def predict_next_word(prefix):\n",
        "    # Tokenize the input prefix\n",
        "    input_ids = tokenizer.encode(prefix, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate predictions\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids, max_length=20, num_return_sequences=1)\n",
        "\n",
        "    print(output)\n",
        "    # Decode and return the predicted word\n",
        "    predicted_word = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage\n",
        "prefix = input(\"Enter the prefix: \")  # Input prefix for prediction\n",
        "predicted_word = predict_next_word(prefix)\n",
        "# predicted_word = predicted_word.replace(prefix, \"\", 1)\n",
        "print(\"Predicted next word:\", predicted_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmXUFMetjdU7",
        "outputId": "32189340-ad24-4d82-eeea-a2423c9c4ac8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the prefix: ref\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5420,   330, 31688, 10659,    43, 50257, 50257, 50257, 50257, 50257,\n",
            "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]])\n",
            "Predicted next word: refacREFACTL\n"
          ]
        }
      ]
    }
  ]
}