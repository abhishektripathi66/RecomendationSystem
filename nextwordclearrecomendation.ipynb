{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJLXnJqJ+cn+U97AlpTHhH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishektripathi66/RecomendationSystem/blob/main/nextwordclearrecomendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBnTUKDHzVyf"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import json\n",
        "import random\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"pizza.txt\", \"r\") as file:   # Unpickling\n",
        "    cleaned_text = file.read()\n",
        "# with open(\"pizza.txt\", \"r\") as file:   # Unpickling\n",
        "#     cleaned_twitter = file.read()\n",
        "cleaned_corpus=cleaned_text\n",
        "print(len(cleaned_corpus))"
      ],
      "metadata": {
        "id": "54i64Ia3zmjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data(length):\n",
        "    seq=[]\n",
        "    start=random.randint(0, len(cleaned_corpus)) # Used half of the corpus due to memory error\n",
        "    for i in range(length,len(cleaned_corpus)):\n",
        "        words = cleaned_corpus[i-length+start:i+start]\n",
        "        print(words)\n",
        "        line = ''.join(words)\n",
        "        if line.strip()!='':\n",
        "          seq.append(line)\n",
        "    with open('len'+str(length)+'.json', 'w') as fp:\n",
        "        json.dump(seq, fp)\n",
        "\n",
        "    del seq"
      ],
      "metadata": {
        "id": "SPu08ScoztpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sequence of length 2\n",
        "create_data(2)"
      ],
      "metadata": {
        "id": "QklnOvIEzwEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_data(4)"
      ],
      "metadata": {
        "id": "i8dwNAiS1Nf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_data(7)"
      ],
      "metadata": {
        "id": "1ZsrXOha1N_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy --upgrade"
      ],
      "metadata": {
        "id": "NRIpqnkxnROG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "import numpy as np\n",
        "def encoding_data(length):\n",
        "    with open('len'+str(length)+'.json', 'r') as fp:\n",
        "        seq=json.load( fp)\n",
        "    print(seq)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(seq)\n",
        "    sequences = tokenizer.texts_to_sequences(seq)\n",
        "    # print(sequences)\n",
        "    length = max(map(len, sequences))\n",
        "    sequences=np.array([xi+[0]*(length-len(xi)) for xi in sequences])\n",
        "    # sequences = np.array([np.array(xi) for xi in sequences])\n",
        "    # flat_array = [item for sublist in sequences for item in sublist]\n",
        "    # sequences = flat_array\n",
        "    # sequences=np.array(sequences)\n",
        "    # print(sequences)\n",
        "    vocab=len(tokenizer.word_counts)+1\n",
        "    data_x=sequences[:,-1]\n",
        "    print(data_x)\n",
        "    data_y=sequences[:,0]\n",
        "    print(data_y)\n",
        "    print(type(data_y))\n",
        "    data_y = to_categorical(data_y, num_classes=vocab)\n",
        "    words_to_index = tokenizer.word_index\n",
        "    with open('tokenizer_len'+str(length)+'.pickle', 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    del seq\n",
        "    return data_x,data_y,vocab,words_to_index"
      ],
      "metadata": {
        "id": "lKrqFFC-hE_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Attention Model for classification\n",
        "#Code Reference below:\n",
        "#https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/#:~:text=Let's%20not%20implement%20a%20simple,Keras%20custom%20layer%20generation%20rule.\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "class attention(Layer):\n",
        "    def init(self):\n",
        "        super(attention,self).__init__()\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name='att_weight',shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name='att_bias',shape=(input_shape[-2],1),initializer=\"zeros\")\n",
        "        super(attention, self).build(input_shape)\n",
        "    def call(self,x):\n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x*a\n",
        "        return K.sum(output, axis=1)"
      ],
      "metadata": {
        "id": "G58kB_oC1VQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_model(length,unit1,n):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from tensorflow.keras.regularizers import l2\n",
        "    from tensorflow.keras.layers import LSTM, Activation, Dropout, Dense, Input,Embedding,Bidirectional\n",
        "    from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,LearningRateScheduler,EarlyStopping\n",
        "    from tensorflow.keras.models import Model,Sequential,save_model\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    # Calling the encoding function to get the data of specified length and the vocabulary\n",
        "    data_x,data_y,v,wti=encoding_data(length)\n",
        "    print(\"Data Encoded\")\n",
        "    print(\"Data_x\",data_x[:5])\n",
        "    print(\"Data_y\",data_y[:5])\n",
        "    print(\"Vocab_Size\",v)\n",
        "\n",
        "    # Preparing the model based on the inputs of unit1 ,unit2 and vocab values\n",
        "    model = Sequential()\n",
        "    #model.add(Embedding(input_dim=v, output_dim=300 , input_length=length-1, weights = [embed_matrix], trainable=False))\n",
        "    model.add(Embedding(v, length-1, input_length=length-1))\n",
        "    model.add(Bidirectional(LSTM(unit1, return_sequences=True)))\n",
        "    model.add(attention())\n",
        "    #model.add(Dropout(0.5))\n",
        "    #model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(v, activation='softmax'))\n",
        "    print(model.summary())\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "    filepath=\"lstmatt_len\"+str(length)+\".hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "    '''\n",
        "    def scheduler(epoch):\n",
        "\n",
        "        if epoch < 60:\n",
        "            return 0.001\n",
        "        elif epoch < 100:\n",
        "            return 0.0005\n",
        "        else:\n",
        "            return 0.0001\n",
        "    '''\n",
        "    #lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=3, min_lr=0.0001,min_delta=0.5,verbose=1)\n",
        "    #es = EarlyStopping(monitor='val_loss', patience=5,min_delta=0.1,verbose=1)\n",
        "    callbacks_list = [checkpoint]\n",
        "    # fit model with epochs 130\n",
        "\n",
        "    history=model.fit(data_x, data_y,batch_size=128, epochs=n,callbacks=callbacks_list)\n",
        "\n",
        "    del data_x,data_y,v,wti\n",
        "\n",
        "    # Plotting the loss and accuracy achieved by the model in each epoch\n",
        "    # fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(20,10))\n",
        "    # ax[0].plot(history.history['loss'])\n",
        "    # ax[1].plot(history.history['accuracy'])\n",
        "    # ax[0].set_title('model loss')\n",
        "    # ax[1].set_title('model Accuracy')\n",
        "    # ax[0].set_ylabel('loss')\n",
        "    # ax[0].set_xlabel('epoch')\n",
        "    # ax[1].set_ylabel('accuracy')\n",
        "    # ax[1].set_xlabel('epoch')\n",
        "    # fig.tight_layout(pad=10.0)\n",
        "    # plt.show()"
      ],
      "metadata": {
        "id": "Lk_vZlZf1a1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here the history is of length 1 ,Bidirectional LSTM layer has 300 units and LSTM layer has 100 units\n",
        "lstm_model(2,32,60)"
      ],
      "metadata": {
        "id": "Nl3McImczzqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here the history is of length 3 ,Bidirectional LSTM layer has 250 units and LSTM layer has 100 units\n",
        "# lstm_model(4,64,80)"
      ],
      "metadata": {
        "id": "4BKG_jcd6glz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here the history is of length 6 ,Bidirectional LSTM layer has 150 units and LSTM layer has 50 units\n",
        "# lstm_model(7,64,70)"
      ],
      "metadata": {
        "id": "ZgKv7_R76lAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "'''\n",
        "start - random integer that indicates the starting index of tokens in the test data\n",
        "temp_length - length of each line in the test corpus from which test data will be created\n",
        "length - length of history + next word in the test data\n",
        "'''\n",
        "def create_testdata2(start,temp_length,length):\n",
        "    seq=[]\n",
        "\n",
        "    for i in range(temp_length,50000):\n",
        "        words = cleaned_corpus[i-temp_length+start:i+start]\n",
        "        line = ' '.join(words)\n",
        "        seq.append(line)\n",
        "        if i % 10000==0:\n",
        "            print(i , 'tokens done')\n",
        "    with open('tokenizer_len'+str(length)+'.pickle', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(seq)\n",
        "    sequences = pad_sequences(sequences, maxlen=length, truncating='pre')\n",
        "    sequences=np.array(sequences)\n",
        "\n",
        "    vocab=len(tokenizer.word_counts)+1\n",
        "    print(vocab)\n",
        "    data_x=sequences[:,:-1]\n",
        "    data_y=sequences[:,-1]\n",
        "    data_y = to_categorical(data_y, num_classes=vocab)\n",
        "    del seq,sequences\n",
        "    return data_x,data_y\n",
        ""
      ],
      "metadata": {
        "id": "oNRoED6AhoTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for generation data for error analysis\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "'''\n",
        "start - random integer that indicates the starting index of tokens in the test data\n",
        "temp_length - length of each line in the test corpus from which test data will be created\n",
        "length - length of history + next word in the test data\n",
        "'''\n",
        "def create_errordata(start,temp_length,length):\n",
        "    seq=[]\n",
        "\n",
        "    for i in range(temp_length,100):\n",
        "        words = cleaned_corpus[i-temp_length+start:i+start]\n",
        "        line = ' '.join(words)\n",
        "        seq.append(line)\n",
        "\n",
        "    with open('tokenizer_len'+str(length)+'.pickle', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(seq)\n",
        "    sequences = pad_sequences(sequences, maxlen=length, truncating='pre')\n",
        "    sequences=np.array(sequences)\n",
        "\n",
        "    vocab=len(tokenizer.word_counts)+1\n",
        "    print(vocab)\n",
        "    data_x=sequences[:,:-1]\n",
        "    data_y=sequences[:,-1]\n",
        "    data_y = to_categorical(data_y, num_classes=vocab)\n",
        "    del seq,sequences\n",
        "    return data_x,data_y\n",
        ""
      ],
      "metadata": {
        "id": "DjOnGZvcz2qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Error Analysis funtion\n",
        "def error_analysis(model,x,y,length):\n",
        "    pred_y=model.predict(x)\n",
        "    error=np.zeros(len(y))\n",
        "    for i in range(len(pred_y)):\n",
        "        for j in range(len(pred_y[i])):\n",
        "            error[i] += -y[i][j]*np.log(pred_y[i][j])\n",
        "\n",
        "\n",
        "    with open('tokenizer_len'+str(length)+'.pickle', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "    x_words=tokenizer.sequences_to_texts(x)\n",
        "\n",
        "    error_df =pd.DataFrame({'x':x_words, 'error':error})\n",
        "    error_df=error_df.sort_values(by='error',ascending=False)\n",
        "    return error_df"
      ],
      "metadata": {
        "id": "7XT4GnHYmond"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function draws some plots using the input text of worst and best predicted data points.\n",
        "def eda_on_error(error_df):\n",
        "    from collections import Counter\n",
        "    import seaborn as sns\n",
        "    error_df['x']=error_df['x'].astype('str')\n",
        "    worst_predicted=error_df.loc[error_df['error']>8]\n",
        "    best_predicted=error_df.loc[error_df['error']<=8]\n",
        "    wordcloud_worst = WordCloud().generate(' '.join(worst_predicted['x']))\n",
        "    wordcloud_best = WordCloud().generate(' '.join(best_predicted['x']))\n",
        "    worst_words = ' '.join(worst_predicted['x']).split(' ')\n",
        "    best_words = ' '.join(best_predicted['x']).split(' ')\n",
        "\n",
        "    worst_counts= Counter(worst_words)\n",
        "    best_counts= Counter(best_words)\n",
        "    fig, axs = plt.subplots(1, 2,figsize=(20,5),gridspec_kw={'width_ratios': [1,2]})\n",
        "\n",
        "    axs[0].imshow(wordcloud_worst, interpolation='bilinear')\n",
        "    axs[0].set_title('Wordcloud of worst input words')\n",
        "    labels, values = zip(*worst_counts.items())\n",
        "    indSort = np.argsort(values)[::-1]\n",
        "    labels = np.array(labels)[indSort]\n",
        "    values = np.array(values)[indSort]\n",
        "    sns.barplot(labels[:10],values[:10],ax=axs[1])\n",
        "    axs[1].set_title('Histogram of top 10 worst input words')\n",
        "    axs[1].set_xlabel('Words')\n",
        "    axs[1].set_ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2,figsize=(20,5),gridspec_kw={'width_ratios': [1,2]})\n",
        "    axs[0].imshow(wordcloud_best, interpolation='bilinear')\n",
        "    axs[0].set_title('Wordcloud of best input words')\n",
        "    labels, values = zip(*best_counts.items())\n",
        "    indSort = np.argsort(values)[::-1]\n",
        "    labels = np.array(labels)[indSort]\n",
        "    values = np.array(values)[indSort]\n",
        "    sns.barplot(labels[:10],values[:10],ax=axs[1])\n",
        "    axs[1].set_title('Histogram of top 10 best input words')\n",
        "    axs[1].set_xlabel('Words')\n",
        "    axs[1].set_ylabel('Frequency')\n",
        "    plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "QtQ93SPSmrKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# file=\"lstmatt_len7.hdf5\"\n",
        "# model_len7 = load_model(file, custom_objects={'attention': attention})\n",
        "# model_len7.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# file=\"lstmatt_len4.hdf5\"\n",
        "# model_len4 = load_model(file, custom_objects={'attention': attention})\n",
        "# model_len4.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "file=\"lstmatt_len2.hdf5\"\n",
        "model_len2 = load_model(file , custom_objects={'attention': attention})\n",
        "model_len2.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Fpslb9mlz5Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the start and temp_length\n",
        "start=random.randint(1, 1000)\n",
        "temp_length=random.randint(7, 10)\n",
        "print(start,temp_length)"
      ],
      "metadata": {
        "id": "xOrUgt1Tz7sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len2_x,len2_y=create_testdata2(start,temp_length,2)\n",
        "# model_len2.evaluate(x=len2_x,y=len2_y)\n",
        "del len2_x,len2_y"
      ],
      "metadata": {
        "id": "OGZ8So-wz99y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len4_x,len4_y=create_testdata2(start,temp_length,4)\n",
        "# model_len4.evaluate(x=len4_x,y=len4_y)\n",
        "# del len4_x,len4_y"
      ],
      "metadata": {
        "id": "3vS_BmRy2W28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len7_x,len7_y=create_testdata2(start,temp_length,7)\n",
        "# model_len7.evaluate(x=len7_x,y=len7_y)\n",
        "# del len7_x,len7_y"
      ],
      "metadata": {
        "id": "eyd2SI0N2ZYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len2_x,len2_y=create_errordata(start,temp_length,2)\n",
        "len2_error=error_analysis(model_len2,len2_x,len2_y,2)\n",
        "del len2_x,len2_y\n",
        "len2_error.head(20)"
      ],
      "metadata": {
        "id": "AufnNMA42cD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eda_on_error(len2_error)"
      ],
      "metadata": {
        "id": "UGdNXzld2euO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len4_x,len4_y=create_errordata(start,temp_length,4)\n",
        "# len4_error=error_analysis(model_len4,len4_x,len4_y,4)\n",
        "# del len4_x,len4_y\n",
        "# len4_error.head(50)"
      ],
      "metadata": {
        "id": "gbvn_xQg2hZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# eda_on_error(len4_error)"
      ],
      "metadata": {
        "id": "V8zce4uk2jwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len7_x,len7_y=create_errordata(start,temp_length,7)\n",
        "# len7_error=error_analysis(model_len7,len7_x,len7_y,7)\n",
        "# del len7_x,len7_y\n",
        "# len7_error.head(50)"
      ],
      "metadata": {
        "id": "ED5e9pLY2loy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "def extra_space(text):\n",
        "    new_text= re.sub(\"\\s+\",\" \",text)\n",
        "    return new_text\n",
        "def sp_charac(text):\n",
        "    new_text=re.sub(\"[^0-9A-Za-z ]\", \"\" , text)\n",
        "    return new_text\n",
        "def tokenize_text(text):\n",
        "    new_text=word_tokenize(text)\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "lny8iMWu0Awj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next():\n",
        "    import tensorflow as tf\n",
        "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from tensorflow.keras.models import load_model\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    import pickle\n",
        "    import time\n",
        "\n",
        "    # with open('tokenizer_len7.pickle', 'rb') as handle:\n",
        "    #     tokenizer_len7 = pickle.load(handle)\n",
        "\n",
        "    # with open('tokenizer_len4.pickle', 'rb') as handle:\n",
        "    #     tokenizer_len4 = pickle.load(handle)\n",
        "    with open('tokenizer_len2.pickle', 'rb') as handle:\n",
        "        tokenizer_len2 = pickle.load(handle)\n",
        "\n",
        "    # file=\"lstmatt_len7.hdf5\"\n",
        "    # model_len7 = load_model(file ,custom_objects={'attention': attention})\n",
        "    # model_len7.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "    # file=\"lstmatt_len4.hdf5\"\n",
        "    # model_len4 = load_model(file, custom_objects={'attention': attention})\n",
        "    # model_len4.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "    file=\"lstmatt_len2.hdf5\"\n",
        "    model_len2 = load_model(file , custom_objects={'attention': attention})\n",
        "    model_len2.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "    while(True):\n",
        "        text=input()\n",
        "        start= time.time()\n",
        "        cleaned_text=extra_space(text)\n",
        "        cleaned_text=sp_charac(cleaned_text)\n",
        "        tokenized=tokenize_text(cleaned_text)\n",
        "\n",
        "        line = ' '.join(tokenized)\n",
        "\n",
        "\n",
        "        if len(tokenized)==1:\n",
        "\n",
        "            encoded_text = tokenizer_len2.texts_to_sequences([line])\n",
        "            pad_encoded = pad_sequences(encoded_text, maxlen=1, truncating='pre')\n",
        "\n",
        "            for i in (model_len2.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
        "\n",
        "\n",
        "                pred_word = tokenizer_len2.index_word[i]\n",
        "                print(\"Next word suggestion:\",pred_word)\n",
        "        # elif len(tokenized)<4:\n",
        "        #     encoded_text = tokenizer_len4.texts_to_sequences([line])\n",
        "        #     pad_encoded = pad_sequences(encoded_text, maxlen=3, truncating='pre')\n",
        "\n",
        "        #     for i in (model_len4.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
        "\n",
        "\n",
        "        #         pred_word = tokenizer_len4.index_word[i]\n",
        "        #         print(\"Next word suggestion:\",pred_word)\n",
        "        # else:\n",
        "        #     encoded_text = tokenizer_len7.texts_to_sequences([line])\n",
        "        #     pad_encoded = pad_sequences(encoded_text, maxlen=6, truncating='pre')\n",
        "\n",
        "        #     for i in (model_len7.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
        "\n",
        "\n",
        "        #         pred_word = tokenizer_len7.index_word[i]\n",
        "        #         print(\"Next word suggestion:\",pred_word)\n",
        "        print('Time taken: ',time.time()-start)\n",
        ""
      ],
      "metadata": {
        "id": "P36goE1Q7bbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_next()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj1Jhgvy7nj-",
        "outputId": "27a5e592-c9ea-4e55-c43e-1c60566bf7a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n",
            "1/1 [==============================] - 1s 875ms/step\n",
            "Next word suggestion: abh\n",
            "Next word suggestion: ab\n",
            "Next word suggestion: is\n",
            "Time taken:  0.945974588394165\n",
            "abhishek\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Next word suggestion: abh\n",
            "Next word suggestion: ab\n",
            "Next word suggestion: is\n",
            "Time taken:  0.10459256172180176\n",
            "abhishek is \n",
            "Time taken:  0.0003516674041748047\n",
            "is\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Next word suggestion: abh\n",
            "Next word suggestion: is\n",
            "Next word suggestion: ab\n",
            "Time taken:  0.10014104843139648\n",
            "wow\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Next word suggestion: abh\n",
            "Next word suggestion: ab\n",
            "Next word suggestion: is\n",
            "Time taken:  0.0868675708770752\n",
            "common\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Next word suggestion: abh\n",
            "Next word suggestion: ab\n",
            "Next word suggestion: is\n",
            "Time taken:  0.08568620681762695\n",
            "good\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Next word suggestion: abh\n",
            "Next word suggestion: ab\n",
            "Next word suggestion: is\n",
            "Time taken:  0.07972955703735352\n",
            "why\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Next word suggestion: abh\n",
            "Next word suggestion: ab\n",
            "Next word suggestion: is\n",
            "Time taken:  0.09282660484313965\n"
          ]
        }
      ]
    }
  ]
}